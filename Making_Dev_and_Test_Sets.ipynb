{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This script reorganizes datasets stored in a Google Cloud Storage (GCS) bucket into training, development, and testing splits. It assumes the folder is already structures as follows:\n",
        "\n",
        "Bucket: 230-project-tiles\n",
        "  - sentinel-tiles\n",
        "      - q1\n",
        "      - q2\n",
        "      - q3\n",
        "      - q4\n",
        "  - folder: mask-tiles\n",
        "      - q1\n",
        "      - q2\n",
        "      - q3\n",
        "      - q4\n",
        "\n",
        "and reorganizes the GCP bucket to be as follows:\n",
        "\n",
        "Bucket: 230-project-tiles\n",
        "  - sentinel-tiles\n",
        "      - dev\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "      - test\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "      - train\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "  - mask-tiles\n",
        "      - dev\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "      - test\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "      - train\n",
        "        - q1\n",
        "        - q2\n",
        "        - q3\n",
        "        - q4\n",
        "\n",
        "This organization allows for the correct execution of the script titled Final U-Net Model.\n",
        "\n",
        "Key Features:\n",
        "1. **Matching Images and Masks**: Matches images with their corresponding masks using filename conventions.\n",
        "2. **Random Splitting**: Randomly divides the dataset into `train`, `dev`, and `test` splits based on user-defined proportions.\n",
        "3. **Split Preservation**: Organizes the splits into subdirectories (e.g., `sentinel-tiles/train/q1/` and `mask-tiles/train/q1/`).\n",
        "4. **Cloud Operations**: Uses GCP's Python client library to copy files to new paths and optionally deletes the originals.\n",
        "\n",
        "Functionality:\n",
        "- Initializes a GCS client and accesses the specified bucket.\n",
        "- Processes data for each quarter, listing and matching image and mask files.\n",
        "- Shuffles and splits the data into train, dev, and test subsets.\n",
        "- Moves the files into appropriate split directories in the same bucket.\n",
        "\n",
        "Parameters:\n",
        "- `bucket_name`: Name of the GCS bucket containing the dataset.\n",
        "- `source_image_prefix`: Prefix path to the image tiles in the bucket.\n",
        "- `source_mask_prefix`: Prefix path to the mask tiles in the bucket.\n",
        "- `splits`: Dictionary defining the proportions of the dataset to allocate to each split (e.g., `{\"train\": 0.8, \"dev\": 0.1, \"test\": 0.1}`).\n",
        "- `seed`: Random seed for reproducibility in dataset splitting.\n",
        "\n",
        "Usage:\n",
        "- Update the `bucket_name`, `source_image_prefix`, and `source_mask_prefix` to match the structure of your GCS bucket.\n",
        "- Define the desired split proportions in the `splits` dictionary.\n",
        "- Execute the script to organize the dataset into training, development, and testing splits.\n",
        "\n",
        "Dependencies:\n",
        "- Requires `google-cloud-storage` library for interacting with GCS.\n",
        "- Assumes a valid GCP authentication JSON file is available and set via `GOOGLE_APPLICATION_CREDENTIALS`."
      ],
      "metadata": {
        "id": "FY2rxcwU1rXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh2-bNB-ji7u",
        "outputId": "b606b492-a3d8-42fc-b8b6-aad9d887108e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing q1/...\n",
            "Finished splitting q1/: 2156 train, 269 dev, 271 test files.\n",
            "Processing q2/...\n",
            "Finished splitting q2/: 882 train, 110 dev, 111 test files.\n",
            "Processing q3/...\n",
            "Finished splitting q3/: 749 train, 93 dev, 95 test files.\n",
            "Processing q4/...\n",
            "Finished splitting q4/: 1076 train, 134 dev, 135 test files.\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path-to-key.json\"\n",
        "\n",
        "# Input Values\n",
        "bucket_name = \"230-project-tiles\" # name of GCP bucket\n",
        "source_image_prefix = \"sentinel-tiles/\" # folder for image tiles\n",
        "source_mask_prefix = \"mask-tiles/\" # folder for mask tiles\n",
        "splits = {\"train\": 0.8, \"dev\": 0.1, \"test\": 0.1} # train, dev, and test splits\n",
        "\n",
        "def reorganize_dataset(bucket_name, source_image_prefix, source_mask_prefix, splits, seed=42):\n",
        "    \"\"\"\n",
        "    Reorganize the GCP bucket dataset into train, dev, and test datasets.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCP bucket.\n",
        "        source_image_prefix (str): Prefix for image files (e.g., 'sentinel-tiles/').\n",
        "        source_mask_prefix (str): Prefix for mask files (e.g., 'mask-tiles/').\n",
        "        splits (dict): Dictionary specifying train, dev, and test splits (e.g., {\"train\": 0.8, \"dev\": 0.1, \"test\": 0.1}).\n",
        "        seed (int): Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    # Initialize GCP storage client\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    random.seed(seed)\n",
        "\n",
        "    for quarter in [\"q1/\", \"q2/\", \"q3/\", \"q4/\"]:\n",
        "        print(f\"Processing {quarter}...\")\n",
        "\n",
        "        # List all images and masks for the quarter\n",
        "        image_blobs = list(bucket.list_blobs(prefix=f\"{source_image_prefix}{quarter}\"))\n",
        "        mask_blobs = list(bucket.list_blobs(prefix=f\"{source_mask_prefix}{quarter}\"))\n",
        "\n",
        "        # Match images with their corresponding masks by filename\n",
        "        mask_dict = {blob.name.split('/')[-1].replace(\"mask_\", \"image_\"): blob for blob in mask_blobs}\n",
        "        matched_files = [(img_blob, mask_dict[img_blob.name.split('/')[-1]]) for img_blob in image_blobs if img_blob.name.split('/')[-1] in mask_dict]\n",
        "\n",
        "        # Shuffle and split files\n",
        "        random.shuffle(matched_files)\n",
        "        total_files = len(matched_files)\n",
        "        train_cutoff = int(total_files * splits[\"train\"])\n",
        "        dev_cutoff = train_cutoff + int(total_files * splits[\"dev\"])\n",
        "\n",
        "        # Create splits\n",
        "        train_files = matched_files[:train_cutoff]\n",
        "        dev_files = matched_files[train_cutoff:dev_cutoff]\n",
        "        test_files = matched_files[dev_cutoff:]\n",
        "\n",
        "        # Move files to new locations\n",
        "        for split_name, split_files in [(\"train\", train_files), (\"dev\", dev_files), (\"test\", test_files)]:\n",
        "            for img_blob, mask_blob in split_files:\n",
        "                # New paths for image and mask files\n",
        "                new_image_path = f\"{source_image_prefix}{split_name}/{quarter}{img_blob.name.split('/')[-1]}\"\n",
        "                new_mask_path = f\"{source_mask_prefix}{split_name}/{quarter}{mask_blob.name.split('/')[-1]}\"\n",
        "\n",
        "                # Copy image and mask files\n",
        "                bucket.copy_blob(img_blob, bucket, new_image_path)\n",
        "                bucket.copy_blob(mask_blob, bucket, new_mask_path)\n",
        "\n",
        "                # Optionally delete the original files\n",
        "                img_blob.delete()\n",
        "                mask_blob.delete()\n",
        "\n",
        "        print(f\"Finished splitting {quarter}: {len(train_files)} train, {len(dev_files)} dev, {len(test_files)} test files.\")\n",
        "\n",
        "# Run the script\n",
        "reorganize_dataset(bucket_name, source_image_prefix, source_mask_prefix, splits)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}